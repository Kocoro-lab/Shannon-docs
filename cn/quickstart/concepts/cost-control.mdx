---
title: "成本控制"
description: "在 Shannon 中管理令牌预算和降低 LLM 成本"
---

## 概述

Shannon 提供全面的成本控制功能，以防止意外的 LLM 费用并优化支出。通过内置的预算执行和智能路由，与简单实现相比，您可以实现 **85-95% 的成本节省**。

## 设置预算

### 令牌限制

限制每个任务消耗的最大令牌数：

```python
client.submit_task(
    query="分析市场趋势",
    config={
        "budget": {
            "max_tokens": 5000  # 硬性限制
        }
    }
)
```

当达到限制时，任务立即终止并返回预算错误。

### 成本限制

设置最大金额：

```python
config={
    "budget": {
        "max_cost_usd": 0.50  # 最多 $0.50
    }
}
```

<Warning>
超过预算限制的任务将失败并显示 `BUDGET_EXCEEDED` 错误。始终为您的用例设置适当的限制。
</Warning>

### 组合限制

同时使用两者进行精细控制：

```python
config={
    "budget": {
        "max_tokens": 10000,
        "max_cost_usd": 1.00
    }
}
```

当**任一**限制达到时，任务停止。

## 模型层级

Shannon 根据能力和成本将模型分为不同层级：

| 层级 | 模型 | 每百万令牌成本 | 用例 |
|------|--------|-------------------|----------|
| **SMALL** | gpt-4o-mini<br/>claude-haiku | $0.15 - $0.25 | 简单查询、高容量 |
| **MEDIUM** | gpt-4o<br/>claude-sonnet | $3.00 - $15.00 | 通用任务 |
| **LARGE** | gpt-4<br/>claude-opus | $15.00 - $75.00 | 复杂推理、关键任务 |

### 显式层级选择

强制使用特定层级：

```python
client.submit_task(
    query="2+2 等于多少？",
    config={"model_tier": "SMALL"}  # 使用最便宜的模型
)
```

<Tip>
对于生产环境，使用 `# 自动选择模式` 配合 `model_tier="SMALL"` 可在简单查询上节省 90% 以上的成本。
</Tip>

## 智能路由器

Shannon 的学习路由器自动选择能够处理每个任务的最便宜模型。

### 工作原理

1. **任务分析**：分析复杂度和所需能力
2. **模型选择**：从最小可行模型开始
3. **质量检查**：验证输出质量
4. **学习**：记住成功的模型-任务配对

### 成本节省

```python
# 没有智能路由
传统：始终使用 GPT-4 → 每个任务 $0.50

# 使用 Shannon 的路由
Shannon:
  - 70% 路由到 gpt-4o-mini → $0.01
  - 25% 路由到 gpt-4o → $0.15
  - 5% 路由到 gpt-4 → $0.50
平均：每个任务 $0.05（节省 90%）
```

### 监控路由器决策

```python
result = handle.wait()
print(f"使用的模型：{result.metrics.token_usage.model}")
print(f"层级：{result.metrics.token_usage.tier}")
print(f"成本：${result.metrics.token_usage.cost_usd}")
```

## 响应缓存

Shannon 缓存 LLM 响应以消除冗余 API 调用：

### 缓存策略

- **键**：`(messages + model + parameters)` 的 SHA256 哈希
- **TTL**：默认 3600 秒（1 小时）
- **存储**：内存 LRU + 可选 Redis 用于分布式缓存
- **命中率**：生产工作负载通常为 30-50%

### 缓存优势

```bash
# 第一次调用：缓存未命中
任务 1："什么是 Python？" → $0.002（LLM 调用）

# 第二次调用：缓存命中
任务 2："什么是 Python？" → $0.000（已缓存）
```

### 监控缓存性能

```python
result = handle.wait()
if result.metrics.cache_hit:
    print("从缓存提供响应（无 LLM 成本）")
else:
    print(f"缓存未命中 - 成本：${result.metrics.token_usage.cost_usd}")
```

## 提供商速率限制

Shannon 自动遵守提供商速率限制：

### 配置的限制

来自 `config/models.yaml`：

```yaml
providers:
  openai:
    rpm: 10000  # 每分钟请求数
    tpm: 2000000  # 每分钟令牌数
  anthropic:
    rpm: 4000
    tpm: 400000
```

### 自动节流

当接近限制时：
1. 对请求进行排队
2. 随时间分散负载
3. 如果可用，回退到替代提供商

## 成本监控

### 跟踪每个任务的支出

```python
result = handle.wait()
print(f"总令牌：{result.metrics.token_usage.total_tokens}")
print(f"提示令牌：{result.metrics.token_usage.prompt_tokens}")
print(f"完成令牌：{result.metrics.token_usage.completion_tokens}")
print(f"成本：${result.metrics.token_usage.cost_usd:.4f}")
```

### 聚合指标

Shannon 在仪表板中跟踪累计成本：
- 按天/周/月的总支出
- 按用户/团队的成本
- 按认知模式的成本
- 令牌使用趋势

访问 http://localhost:2111 查看实时成本分析。

## 最佳实践

### 1. 始终设置预算

永远不要在没有预算限制的情况下运行生产任务：

```python
# ❌ 不好：无预算限制
client.submit_task(query="...")

# ✅ 好：预算保护
client.submit_task(
    query="...",
    # 通过 .env 配置预算
)
```

### 2. 尽可能使用简单模式

复杂模式成本更高：

```python
# 简单查询：使用简单模式
client.submit_task(
    query="法国的首都是什么？",
    # 自动选择模式  # 单智能体，最少令牌
)

# 复杂查询：使用标准/复杂模式
client.submit_task(
    query="研究并比较 5 种数据库技术",
    # 自动选择模式  # 任务分解是合理的
)
```

### 3. 利用缓存

对于重复查询，使用一致的措辞以最大化缓存命中：

```python
# ❌ 不好：不同措辞阻止缓存命中
client.submit_task(query="什么是 Python？")
client.submit_task(query="告诉我关于 Python 的事")
client.submit_task(query="解释 Python")

# ✅ 好：一致的查询命中缓存
standard_query = "什么是 Python？"
client.submit_task(query=standard_query)  # 缓存未命中
client.submit_task(query=standard_query)  # 缓存命中
client.submit_task(query=standard_query)  # 缓存命中
```

### 4. 监控和优化

定期查看成本指标：

```python
# 启用详细日志
import logging
logging.basicConfig(level=logging.INFO)

# 跟踪成本
total_cost = 0
for task in tasks:
    result = handle.wait()
    cost = result.metrics.token_usage.cost_usd
    total_cost += cost
    print(f"任务：${cost:.4f}，累计：${total_cost:.4f}")
```

### 5. 首先使用较小的模型

让智能路由器证明何时需要更大的模型：

```python
# 让 Shannon 选择
client.submit_task(query="...")  # 自动选择层级

# 或从小开始
client.submit_task(
    query="...",
    config={"model_tier": "SMALL"}
)
```

## 成本优化清单

<Accordion title="优化清单">
  - [ ] 在所有任务上设置 `max_cost_usd`
  - [ ] 对直接查询使用 `# 自动选择模式`
  - [ ] 启用响应缓存（默认：已启用）
  - [ ] 在适当时使用 `model_tier="SMALL"`
  - [ ] 标准化查询措辞以实现缓存命中
  - [ ] 在仪表板中监控成本指标
  - [ ] 设置预算警报（通过 Prometheus）
  - [ ] 审查和优化提示模板
  - [ ] 使用会话上下文减少令牌使用
  - [ ] 启用学习路由器（默认：已启用）
</Accordion>

## 示例：成本优化工作流

```python
from shannon import ShannonClient

client = ShannonClient()

# 高容量、简单查询
simple_tasks = [
    "分类情感：很棒的产品！",
    "分类情感：糟糕的体验",
    "分类情感：还可以"
]

total_cost = 0
for query in simple_tasks:
    handle = client.submit_task(query=query)
    status = client.wait(handle.task_id)
    if status.metrics:
        total_cost += status.metrics.cost_usd

print(f"3 个任务的总成本：${total_cost:.4f}")
# 预期：约 $0.006（与 GPT-4 相比节省 90%）
```

## 下一步

<CardGroup cols={2}>
  <Card title="流式传输事件" icon="stream" href="/cn/quickstart/concepts/streaming">
    实时任务监控
  </Card>
  <Card title="配置" icon="gear" href="/cn/quickstart/configuration">
    高级成本设置
  </Card>
  <Card title="API 概述" icon="code" href="/cn/api/overview">
    端点和用法
  </Card>
  <Card title="监控" icon="chart-line" href="/cn/quickstart/concepts/monitoring">
    成本监控 UI
  </Card>
</CardGroup>


---

## 参与翻译

如果您想帮助翻译此文档，请访问我们的 [GitHub 仓库](https://github.com/Kocoro-lab/Shannon)。
